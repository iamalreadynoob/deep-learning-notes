{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyManvR6Ry5NB16u9oZc29kf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Hello World\n","\n","* Solving MNIST as the “Hello World” of deep learning.\n","\n","***\n","\n","# Basic Terms\n","\n","* **Class**: It refers to a _category_ in a classification.\n","\n","* **Sample**: These are the data points.\n","\n","* **Label**: The class associated with a specific _sample_ is called a _label_.\n","\n","***\n","\n","# Tensors\n","\n","* **Scalars** are _rank-0_, **Vectors** are _rank-1_ and **Matrices** are _rank-2_ tensors.\n","\n","* In deep learning, most of all the time are worked with tensors which have 0 to 4 ranks. Although, it may up to 5 if video data is being processed.\n","\n","* A tensor is defined with three key attributes: _rank_, _shape_ and _data type_.\n","\n","  * **Rank**: This is also called the tensor's **ndim** in Python libraries such as NumPy or TensorFlow.\n","\n","  * **Shape**: This is a tuple of integers that describes how many dimensions the tensor has along each axis.\n","\n","  * **Data Type**: This is the type of the data contained in the tensor; for instance, a tensor's type could be _float16_, _float32_ _float64_, _uint8_, and so on. In TensorFlow, also there are _string_ tensors. It is usually called _dtype_ in Python libraries.\n","\n","***\n","\n","# Data Batches\n","\n","* In general, the first axis in all data tensors is accepted in deep learning as the _samples axis_ (sometimes called the _samples dimension_)\n","\n","* In addition, deep learning models don’t process an entire dataset at once; rather, they break the data into small batches.\n","\n","***\n","\n","# Data Tensors\n","\n","* **Vector Data**: (samples, features)\n","* **Timeseries or Sequence**: (samples, timesteps, features)\n","* **Images**: (samples, height, width, channels)\n","* **Video**: (samples, frames, height, width, channels)\n","\n","***\n","\n","# Tensor Operations\n","\n","* Dense layers are the layers which are connected between them.\n","* A relu operation: relu(x) is max(x, 0); “**relu**” stands for “**rectified linear unit**”\n","\n","  ## Element Wise Operations\n","\n","  * They are operations that are applied independently to each entry in the tensors being considered.\n","  * This means these operations are highly amenable to massively parallel implementations.\n","  * In practice, when dealing with NumPy arrays, these operations are available as well-optimized built-in NumPy functions, which themselves delegate the heavy lifting to a Basic Linear Algebra Subprograms (BLAS) implementation. BLAS are low-level, highly parallel, efficient tensor-manipulation routines that are typically implemented in Fortran or C.\n","  * Likewise, when running TensorFlow code on a GPU, element-wise operations are executed via fully vectorized CUDA implementations that can best utilize the highly parallel GPU chip architecture.\n","\n","  ## Broadcasting\n","\n","  * What happens with addition when the shapes of the two\n","  tensors being added differ? When possible, and if there’s no ambiguity, the smaller tensor will be broadcast to match the shape of the larger tensor. Broadcasting consists of two steps:\n","\n","    1- Axes are added to the smaller tensor to match the ndim of the larger tensor.\n","\n","    2- The smaller tensor is repeated alongside these new axes to match the full shape of the larger tensor.\n","\n","  ## Tensor Product\n","\n","  * It is also called as **dot product**.\n","\n","  ```python\n","  x = np.random.random((32,))\n","  y = np.random.random((32,))\n","  z = np.dot(x, y)\n","  ```\n","\n","  ## Tensor Reshaping\n","\n","  * Reshaping a tensor means rearranging its rows and columns to match a target shape.\n","\n","  ***\n","\n","  # Gradient Bases Optimization\n","\n","  ```python\n","  output = relu(dot(input, W) + b)\n","  ```\n","\n","  * In this expression, W and b are tensors that are attributes of the layer. They’re called the **weights** or **trainable parameters** of the layer.\n","\n","  * These weights contain the information learned by the model from exposure to training data.\n","\n","  * Initially, these weight matrices are filled with small random values.\n","\n","  * What comes next is to gradually adjust these weights, based on a feedback signal. This gradual adjustment, also called **training**, is the learning that machine learning is all about. This happens within what’s called a **training loop**, which works as follows.\n","\n","  * Given a differentiable function, it’s theoretically possible to find its minimum analytically: it’s known that a function’s minimum is a point where the derivative is 0, so all you have to do is find all the points where the derivative goes to 0 and check for which of these points the function has the lowest value. Applied to a neural network, that means finding analytically the combination of weight values that yields the smallest possible loss function.\n","\n","  * Applying the chain rule to the computation of the gradient values of a neural network gives rise to an algorithm called **backpropagation**."],"metadata":{"id":"sE-MUyBHKTQl"}}]}