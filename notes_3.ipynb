{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Layers\n",
        "\n",
        "* Everything in Keras is either a _Layer_ or something that\n",
        "closely interacts with a _Layer_.\n",
        "\n",
        "* A _Layer_ is an object that encapsulates some state (weights) and some computation (a forward pass).\n",
        "\n",
        "* When implementing your own layers, put the forward pass in the _call()_ method.\n",
        "\n",
        "> A deep learning model is a graph of layers. In Keras, that’s the Model class. Until\n",
        "now, you’ve only seen Sequential models (a subclass of Model), which are simple\n",
        "stacks of layers, mapping a single input to a single output. But as you move forward,\n",
        "you’ll be exposed to a much broader variety of network topologies. These are some\n",
        "common ones:\n",
        "\n",
        "  * two-branch networks\n",
        "  * multihead networks\n",
        "  * residual connections\n",
        "\n",
        "* The compile() method configures the training process. It takes the arguments optimizer, loss, and metrics (a list):\n",
        "\n",
        "```python\n",
        "model = keras.Sequential([keras.layers.Dense(1)])\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"mean_squared_error\",\n",
        "              metrics=[\"accuracy\"])\n",
        "```\n",
        "\n",
        "* The fit() method implements the training loop itself.\n",
        "These are its key arguments:\n",
        "  * The data (inputs and targets) to train on. It will typically be passed either in the\n",
        "  form of NumPy arrays or a TensorFlow Dataset object. You’ll learn more about\n",
        "the Dataset API in the next chapters.\n",
        "  * The number of epochs to train for: how many times the training loop should iter-\n",
        "ate over the data passed.\n",
        "  * The batch size to use within each epoch of mini-batch gradient descent: the\n",
        "number of training examples considered to compute the gradients for one\n",
        "weight update step.\n",
        "\n",
        "***\n",
        "\n",
        "# Optimizers In Keras\n",
        "\n",
        "* SGD\n",
        "* RMSprop\n",
        "* Adam\n",
        "* Adagrad\n",
        "* etc.\n",
        "\n",
        "# Losses In Keras\n",
        "\n",
        "* CategoricalCrossentropy\n",
        "* SparseCategoricalCrossentropy\n",
        "* BinaryCrossentropy\n",
        "* MeanSquaredError\n",
        "* KLDivergence\n",
        "* CosineSimilarity\n",
        "* etc.\n",
        "\n",
        "# Metrics In Keras\n",
        "\n",
        "* CategoricalAccuracy\n",
        "* SparseCategoricalAccuracy\n",
        "* BinaryAccuracy\n",
        "* AUC\n",
        "* Precision\n",
        "* Recall\n",
        "* etc.\n",
        "\n",
        "***\n",
        "\n",
        "* Once you’ve trained your model, you’re going to want to use it to make predictions\n",
        "on new data. This is called _inference_.\n",
        "\n",
        "```python\n",
        "predictions = model(new_inputs)\n",
        "```\n",
        "\n",
        "* However, this will process all inputs in new_inputs at once, which may not be feasible\n",
        "if you’re looking at a lot of data\n",
        "\n",
        "* A better way to do inference is to use the **predict()** method.\n",
        "\n",
        "```python\n",
        "predictions = model.predict(new_inputs, batch_size=128)\n",
        "```"
      ],
      "metadata": {
        "id": "oFZalvZ6D_jX"
      }
    }
  ]
}